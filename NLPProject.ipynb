{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPProject.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishalveerareddy/NLP-Project/blob/main/NLPProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9lU66OjC4Qo",
        "outputId": "98305cb6-a7d1-4da4-aef2-40b212e772c1"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.1.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing<3,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "an8VnfFkC6DZ"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PA1lKgqYyD3i"
      },
      "source": [
        "from transformers import BertForQuestionAnswering\n",
        "\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwVWTRIxC-po"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LG7y0ZQEC-75"
      },
      "source": [
        "def answer_question(question, answer_text):\n",
        "    # ======== Tokenize ========\n",
        "    # Apply the tokenizer to the input text, treating them as a text-pair.\n",
        "    input_ids = tokenizer.encode(question, answer_text)\n",
        "\n",
        "    # Report how long the input sequence is.\n",
        "    print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n",
        "\n",
        "    # ======== Set Segment IDs ========\n",
        "    # Search the input_ids for the first instance of the `[SEP]` token.\n",
        "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
        "\n",
        "    # The number of segment A tokens includes the [SEP] token istelf.\n",
        "    num_seg_a = sep_index + 1\n",
        "\n",
        "    # The remainder are segment B.\n",
        "    num_seg_b = len(input_ids) - num_seg_a\n",
        "\n",
        "    # Construct the list of 0s and 1s.\n",
        "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        "\n",
        "    # There should be a segment_id for every input token.\n",
        "    assert len(segment_ids) == len(input_ids)\n",
        "\n",
        "    # ======== Evaluate ========\n",
        "    # Run our example through the model.\n",
        "    outputs = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
        "                    token_type_ids=torch.tensor([segment_ids]), # The segment IDs to differentiate question from answer_text\n",
        "                    return_dict=True) \n",
        "\n",
        "    start_scores = outputs.start_logits\n",
        "    end_scores = outputs.end_logits\n",
        "\n",
        "    # ======== Reconstruct Answer ========\n",
        "    # Find the tokens with the highest `start` and `end` scores.\n",
        "    answer_start = torch.argmax(start_scores)\n",
        "    answer_end = torch.argmax(end_scores)\n",
        "\n",
        "    # Get the string versions of the input tokens.\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "    # Start with the first token.\n",
        "    answer = tokens[answer_start]\n",
        "\n",
        "    # Select the remaining answer tokens and join them with whitespace.\n",
        "    for i in range(answer_start + 1, answer_end + 1):\n",
        "        \n",
        "        # If it's a subword token, then recombine it with the previous token.\n",
        "        if tokens[i][0:2] == '##':\n",
        "            answer += tokens[i][2:]\n",
        "        \n",
        "        # Otherwise, add a space then the token.\n",
        "        else:\n",
        "            answer += ' ' + tokens[i]\n",
        "\n",
        "    print('Answer: \"' + answer + '\"')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "5ch9sAHjDBef",
        "outputId": "357cdef4-015e-481b-eb28-162296e38f63"
      },
      "source": [
        "import textwrap\n",
        "\n",
        "wrapper = textwrap.TextWrapper(width=80) \n",
        "\n",
        "bert_abstract = \"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\"\n",
        "\n",
        "bert_abstract"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPRz4PM-DCoP",
        "outputId": "e2a0fc81-fd1d-4e60-c55f-e123ca3d3ca1"
      },
      "source": [
        "question = \"What does the 'B' in BERT stand for?\"\n",
        "\n",
        "answer_question(question, bert_abstract)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query has 258 tokens.\n",
            "\n",
            "Answer: \"bidirectional encoder representations from transformers\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NbRSDEoDELU",
        "outputId": "8e10fb24-2967-4db5-dd5b-71bdc1d79a7f"
      },
      "source": [
        "question = \"Is BERT simple?\"\n",
        "\n",
        "answer_question(question, bert_abstract)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query has 251 tokens.\n",
            "\n",
            "Answer: \"conceptually simple and empirically powerful\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIsMjEN8DFMy"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4SN2nWEk0wS"
      },
      "source": [
        "with open('train_qa.txt','rb') as f:\n",
        "    train_data=pickle.load(f)\n",
        "with open(\"test_qa.txt\",'rb') as f:\n",
        "  test_data = pickle.load(f)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3s-9EPRoRtaS"
      },
      "source": [
        "data = train_data + test_data\n",
        "\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeTKMoXnFUYj"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hFySgtxRuso"
      },
      "source": [
        "# Baseline\n",
        "#utility function for BERT word embeddings\n",
        "def bert_vectorize(sentences):\n",
        "    #load pretrained BERT model\n",
        "    model = SentenceTransformer('bert-base-nli-stsb-mean-tokens')\n",
        "    #encode sentences\n",
        "    vectors = model.encode(sentences)\n",
        "    return list(vectors)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WT3uEe-HScRm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b2dd535-c168-4c12-fb38-ed7ff1d2dbd7"
      },
      "source": [
        "!pip install sentence_transformers\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "Requirement already satisfied: tokenizers>=0.10.3 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.10.3)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.1.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.12.3)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.19.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.11.1+cu111)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.62.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.4.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.1.96)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence_transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (4.8.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (21.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.0.46)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2.23.0)\n",
            "Requirement already satisfied: pyparsing<3,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt6CcQ8FpXEb",
        "outputId": "367b35e1-6c4e-4228-ad50-c7a66992ab0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "class EmbeddingVectorizer(object):\n",
        "    def __init__(self, word2vec):\n",
        "          self.word2vec = word2vec\n",
        "          self.dim = len(next(iter(word2vec.values())))\n",
        "    def fit(self, X, y):\n",
        "            return self\n",
        "    def transform(self, X):\n",
        "            return np.array([\n",
        "                np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
        "                        or [np.zeros(self.dim)], axis=0)\n",
        "                for words in X\n",
        "            ])\n",
        "def w2v(X_train, X_test):\n",
        "  model = gensim.models.Word2Vec([doc for i, doc in enumerate(X_train+X_test)], min_count = 1, \n",
        "                              size = 100, window = 5)\n",
        "\n",
        "  scaler = MinMaxScaler()\n",
        "\n",
        "\n",
        "  d2v = dict(zip(model.wv.index2word, model.wv.syn0)) \n",
        "  modelw = EmbeddingVectorizer(d2v)\n",
        "  # converting text to numerical data using Word2Vec \n",
        "  X_train_vectors_w2v = modelw.transform(X_train)\n",
        "  X_train_vec = scaler.fit_transform(X_train_vectors_w2v)#Used for normalising the vector\n",
        "\n",
        "  X_test_vectors_w2v = modelw.transform(X_test)\n",
        "  X_test_vec = scaler.fit_transform(X_test_vectors_w2v)#Used for normalising the vector\n",
        "  return [X_train_vec,X_test_vec]\n",
        "\n",
        "  "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2NLcwZoEFm1"
      },
      "source": [
        "train_story_text=[]\n",
        "train_question_text=[]\n",
        "train_answers=[]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Xjp_3rjR9Mr"
      },
      "source": [
        "for story,question,answer in data:\n",
        "    train_story_text.append(story)\n",
        "    train_question_text.append(question)\n",
        "    train_answers.append(answer)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDo8kQtTSAi0"
      },
      "source": [
        "sq_pairs = [str(train_story_text[i])+\" \"+str(train_question_text[i]) for i in range(len(train_story_text))]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MezOHDDo1Kaf",
        "outputId": "fd1ba364-b0fc-4fde-d78e-76f1a21709f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "word = []\n",
        "for statement in sq_pairs:\n",
        "  st = word_tokenize(statement)\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  finalStats =[]\n",
        "  for x in st:\n",
        "    \n",
        "    finalStats.append(lemmatizer.lemmatize(x))\n",
        "  word.append(finalStats)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fwzls6YS1XoJ",
        "outputId": "304dfabf-eb6e-480e-e1b8-ee704919fce6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "word[0:5]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['[',\n",
              "  \"'Mary\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'moved\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'to\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'the\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'bathroom\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'\",\n",
              "  '.',\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'Sandra\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'journeyed\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'to\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'the\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'bedroom\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'\",\n",
              "  '.',\n",
              "  \"'\",\n",
              "  ']',\n",
              "  '[',\n",
              "  \"'Is\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'Sandra\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'in\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'the\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'hallway\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'\",\n",
              "  '?',\n",
              "  \"'\",\n",
              "  ']'],\n",
              " ['[',\n",
              "  \"'Mary\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'moved\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'to\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'the\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'bathroom\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'\",\n",
              "  '.',\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'Sandra\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'journeyed\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'to\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'the\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'bedroom\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'\",\n",
              "  '.',\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'Mary\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'went\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'back\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'to\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'the\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'bedroom\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'\",\n",
              "  '.',\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'Daniel\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'went\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'back\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'to\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'the\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'hallway\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'\",\n",
              "  '.',\n",
              "  \"'\",\n",
              "  ']',\n",
              "  '[',\n",
              "  \"'Is\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'Daniel\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'in\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'the\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'bathroom\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'\",\n",
              "  '?',\n",
              "  \"'\",\n",
              "  ']'],\n",
              " ['[',\n",
              "  \"'Mary\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'moved\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'to\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'the\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'bathroom\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'\",\n",
              "  '.',\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'Sandra\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'journeyed\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'to\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'the\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'bedroom\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'\",\n",
              "  '.',\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'Mary\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'went\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'back\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'to\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'the\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'bedroom\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'\",\n",
              "  '.',\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'Daniel\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'went\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'back\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'to\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'the\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'hallway\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'\",\n",
              "  '.',\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'Sandra\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'went\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'to\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'the\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'kitchen\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'\",\n",
              "  '.',\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'Daniel\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'went\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'back\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'to\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'the\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'bathroom\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'\",\n",
              "  '.',\n",
              "  \"'\",\n",
              "  ']',\n",
              "  '[',\n",
              "  \"'Is\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'Daniel\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'in\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'the\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'office\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'\",\n",
              "  '?',\n",
              "  \"'\",\n",
              "  ']'],\n",
              " ['[',\n",
              "  \"'Mary\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'moved\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'to\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'the\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'bathroom\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'\",\n",
              "  '.',\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'Sandra\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'journeyed\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'to\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'the\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'bedroom\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'\",\n",
              "  '.',\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'Mary\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'went\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'back\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'to\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'the\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'bedroom\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'\",\n",
              "  '.',\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'Daniel\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'went\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'back\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'to\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'the\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'hallway\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'\",\n",
              "  '.',\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'Sandra\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'went\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'to\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'the\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'kitchen\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'\",\n",
              "  '.',\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'Daniel\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'went\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'back\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'to\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'the\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'bathroom\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'\",\n",
              "  '.',\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'Daniel\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'picked\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'up\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'the\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'football\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'there\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'\",\n",
              "  '.',\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'Daniel\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'went\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'to\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'the\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'bedroom\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'\",\n",
              "  '.',\n",
              "  \"'\",\n",
              "  ']',\n",
              "  '[',\n",
              "  \"'Is\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'Daniel\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'in\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'the\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'bedroom\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'\",\n",
              "  '?',\n",
              "  \"'\",\n",
              "  ']'],\n",
              " ['[',\n",
              "  \"'Mary\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'moved\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'to\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'the\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'bathroom\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'\",\n",
              "  '.',\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'Sandra\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'journeyed\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'to\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'the\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'bedroom\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'\",\n",
              "  '.',\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'Mary\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'went\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'back\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'to\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'the\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'bedroom\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'\",\n",
              "  '.',\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'Daniel\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'went\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'back\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'to\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'the\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'hallway\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'\",\n",
              "  '.',\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'Sandra\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'went\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'to\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'the\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'kitchen\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'\",\n",
              "  '.',\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'Daniel\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'went\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'back\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'to\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'the\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'bathroom\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'\",\n",
              "  '.',\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'Daniel\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'picked\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'up\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'the\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'football\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'there\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'\",\n",
              "  '.',\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'Daniel\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'went\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'to\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'the\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'bedroom\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'\",\n",
              "  '.',\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'John\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'travelled\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'to\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'the\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'office\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'\",\n",
              "  '.',\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'Sandra\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'went\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'to\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'the\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'garden\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'\",\n",
              "  '.',\n",
              "  \"'\",\n",
              "  ']',\n",
              "  '[',\n",
              "  \"'Is\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'Daniel\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'in\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'the\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'bedroom\",\n",
              "  \"'\",\n",
              "  ',',\n",
              "  \"'\",\n",
              "  '?',\n",
              "  \"'\",\n",
              "  ']']]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATCa7B05SC8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "36349224-01f9-4484-99a4-578421fa06f6"
      },
      "source": [
        "#get word embeddings using BERT\n",
        "vectorized_sq_pairs = bert_vectorize(sq_pairs)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-8ca0dec1887a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#get word embeddings using BERT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvectorized_sq_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_vectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msq_pairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-f98ea350ce0a>\u001b[0m in \u001b[0;36mbert_vectorize\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-nli-stsb-mean-tokens'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#encode sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;31m# fixes for #522 and #487 to avoid oom problems on gpu with large datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mconvert_to_numpy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0mall_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mN_Xma5QSz3X"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X= word\n",
        "y=train_answers\n",
        "\n",
        "# X_train,X_test = w2v(X_train=X_train[0:5],X_test= X_test[0:5])\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lH6SeXruY7v"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsxWAO4DvzQH",
        "outputId": "7f0d72bd-b928-4645-fdb3-4bae8ea68334",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_train,X_test = w2v(X_train=X_train,X_test= X_test)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghgpDiueS80g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34a6c00d-1171-4b0f-ef57-1f2628b71c8e"
      },
      "source": [
        "#classify response type\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "svmmodel = SVC(kernel='linear', degree=3, C=1, decision_function_shape='ovo').fit(X_train, y_train)\n",
        "predictions = svmmodel.predict(X_test)\n",
        "accuracy = svmmodel.score(X_test, y_test)\n",
        "f1score = f1_score(y_test, predictions, average='micro')\n",
        "print(classification_report(y_test, predictions))\n",
        "print(\"Accuracy of SVM with Polynomial Kernel:\", accuracy)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          no       0.49      0.58      0.53      1803\n",
            "         yes       0.49      0.40      0.44      1827\n",
            "\n",
            "    accuracy                           0.49      3630\n",
            "   macro avg       0.49      0.49      0.48      3630\n",
            "weighted avg       0.49      0.49      0.48      3630\n",
            "\n",
            "Accuracy of SVM with Polynomial Kernel: 0.48650137741046834\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2eNK0O0TD0r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}